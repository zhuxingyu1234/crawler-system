

以下是为你的爬虫添加 **随机IP代理自动重启与切换** 的代码修改方案，基于常见的 `scrapy-rotating-proxies` 扩展和自定义逻辑：

---

### **步骤 1：安装必要的依赖**
```bash
# 在你的 requirements.txt 添加
scrapy-rotating-proxies==0.8.3
scrapy-user-agents==0.1.1
```

---

### **步骤 2：在 `settings.py` 中配置随机代理**
```python
# settings.py 添加以下内容

# 启用随机代理中间件
DOWNLOADER_MIDDLEWARES = {
    'scrapy.downloadermiddlewares.retry.RetryMiddleware': 90,
    'scrapy_rotating_proxies.middlewares.RotatingProxyMiddleware': 100,
    'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware': 110,
    # 如果同时需要随机User-Agent（推荐配合代理）
    'scrapy_user_agents.middlewares.RandomUserAgentMiddleware': 400,
}

# 代理源设置（根据实际代理类型修改）
ROTATING_PROXY_LIST = [
    'http://user:pass@proxy1:port',
    'http://user:pass@proxy2:port',
    # ...其他代理
]

# 或者从文件动态加载代理（推荐，更灵活）
# ROTATING_PROXY_LIST_PATH = 'proxies.txt'  # 每行一个代理地址

# 代理失败自动重试策略
ROTATING_PROXY_PAGE_RETRY_TIMES = 5  # 单个代理重试次数（超频次后会自动更换）
ROTATING_PROXY_CLOSE_SPIDER = False   # 当所有代理失效时不关闭爬虫

# 代理验证超时（秒）
ROTATING_PROXY_BACKOFF_BASE = 5  
DOWNLOAD_TIMEOUT = 30

# User-Agent 随机化（需 scrapy-user-agents）
USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) ...',
    # ...其他 UA
]
```

---

### **步骤 3：创建动态代理文件（可选）**
如果你选择从文件加载代理（`proxies.txt`）：
```bash
# 示例内容（每行一个代理）
http://user1:pass1@ip1:port1
http://user2:pass2@ip2:port2
socks5://user3:pass3@ip3:port3
```

**动态刷新代理**：可以在定时任务中更新此文件（如通过代理商的API拉取新IP），爬虫会主动检测文件变化并加载新代理。

---

### **步骤 4：增强代理错误处理**
在中间件中捕获代理错误并触发代理重启（自定义 `middlewares.py`）：
```python
# middlewares.py
from scrapy.exceptions import NotConfigured
from scrapy_rotating_proxies.expire import Proxies, exp_backoff_full_jitter

class CustomProxyMiddleware:
    def __init__(self, proxy_list):
        self.proxies = Proxies(proxy_list)
        self.backoff = exp_backoff_full_jitter()

    @classmethod
    def from_crawler(cls, crawler):
        if not crawler.settings.get('ROTATING_PROXY_LIST'):
            raise NotConfigured
        return cls(crawler.settings.get('ROTATING_PROXY_LIST'))

    def process_request(self, request, spider):
        proxy = self.proxies.get_random()
        if proxy:
            request.meta['proxy'] = proxy
        else:
            spider.logger.warning("无可用代理，尝试重启代理库...")
            self._refresh_proxies(spider)
            # 递归重试（或调用代理更新API）
            return request.copy()

    def process_exception(self, request, exception, spider):
        proxy = request.meta.get('proxy')
        if proxy:
            self.proxies.mark_dead(proxy)
            spider.logger.debug(f'代理 {proxy} 标记为失效，剩余代理数: {len(self.proxies)}')
            # 自动重启逻辑
            if len(self.proxies) < 3:  # 当代理池少于阈值时触发重启
                self._refresh_proxies(spider)

    def _refresh_proxies(self, spider):
        """重启代理库：从外部API或文件重新加载"""
        # 示例：调用代理API获取新地址（需实现）
        new_proxies = self._fetch_new_proxies()
        self.proxies = Proxies(new_proxies)
        spider.logger.info(f'代理库已刷新，当前可用代理数: {len(self.proxies)}')

    def _fetch_new_proxies(self):
        # 示例：调用代理服务商API（需要根据实际API文档编写）
        # 例如：response = requests.get('http://proxy-provider.com/api?count=20')
        # return [line.strip() for line in response.text.split('\n') if line.strip()]
        return []  # 返回实际代理地址列表
```

---

### **步骤 5：在爬虫中触发动态代理刷新**
在 `spiders/main_spider.py` 中添加定时任务：
```python
from twisted.internet import task

class DistributedSpider(RedisSpider):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        # 每1小时主动刷新代理（根据需求调整间隔）
        self.proxy_refresh_task = task.LoopingCall(self.refresh_proxies)
        self.proxy_refresh_task.start(3600)

    def refresh_proxies(self):
        """调用中间件的代理刷新方法"""
        self.crawler.engine.downloader.middleware.middlewares[0]._refresh_proxies(self)

    def spider_closed(self):
        self.proxy_refresh_task.stop()
```

---

### **参数调优建议**

| **配置项**                       | **说明**                                                                 |
|----------------------------------|-------------------------------------------------------------------------|
| `ROTATING_PROXY_PAGE_RETRY_TIMES` | 同一个代理的重试次数，建议设为3-5次                                           |
| `DOWNLOAD_TIMEOUT`               | 单次请求超时时间，根据代理延迟调整（建议设为20-60秒）                              |
| `ROTATING_PROXY_BACKOFF_BASE`    | 代理失败后的退避时间基数（避免频繁重试相同代理），公式为 `base * 2^(尝试次数-1)` 秒       |
| **定时刷新间隔**                 | 根据代理商的IP存活时间调整（如短效代理设为5-30分钟）                               |

---

### **验证代理切换**
1. 启动爬虫并观察日志：
   ```
   [scrapy-rotating-proxies] 代理列表已加载，共10个
   [scrapy-rotating-proxies] 使用代理：http://ip1:port1
   [scrapy-rotating-proxies] 代理失效，剩余9个活代理（触发 _refresh_proxies）
   ```

2. 手动修改/动态更新 `proxies.txt` 文件，观察爬虫是否自动加载新代理。

---

通过以上修改，你的爬虫将在以下情况下自动重启/切换代理：
1. **代理连续失败超过阈值**  
2. **代理池数量低于安全水位**  
3. **定时刷新触发**